# Architectural Review Complete: ETL Multi-Source Design

**Date:** February 15, 2026  
**Status:** âœ… ARCHITECTURE DESIGN COMPLETE & READY FOR IMPLEMENTATION  
**Prepared By:** Architect Review  

---

## Executive Summary

I have completed a comprehensive architectural review of the Draft Queen platform's data integration strategy. The current monolithic approach is **not scalable** beyond 2-3 data sources and **lacks auditability**.

**Recommendation:** Implement an **enterprise-grade ETL architecture** with:
- Source-specific staging tables (raw data capture)
- Canonical transformation layer (normalized business entities)  
- Complete data lineage tracking (audit trail)
- Parallel extraction + atomic loading

**Timeline:** 4-5 weeks to production
**Complexity:** Medium (new patterns, well-designed)
**Team:** Data Engineer + Backend Engineer
**Benefit:** Scalable, auditable, maintainable system for 5+ data sources

---

## What Was Delivered

### 1. **Architecture Decision Record (ADR 0011)**
ðŸ“„ File: [`docs/architecture/0011-etl-multi-source-architecture.md`](docs/architecture/0011-etl-multi-source-architecture.md)

**700 lines covering:**
- Complete physical data layer (5 layers: extract â†’ stage â†’ transform â†’ load â†’ publish)
- Source-specific staging table schemas (PFF, NFL, CFR, Yahoo, ESPN)
- Canonical transformation models (identity, grades, measurements, stats)
- Data lineage audit trail design
- ETL orchestration flow with Python pseudocode
- Example transformation rules
- Storage/performance analysis with mitigation
- 4-phase implementation roadmap
- Risk assessment and trade-offs

**Key Sections:**
- Physical data layer architecture diagram
- Source-specific staging tables (SQL DDL)
- Canonical models (prospect_core, prospect_grades, etc.)
- Data lineage table (complete audit trail)
- ETL orchestration pseudocode
- Alternative approaches considered + rejected

### 2. **Implementation Guide**
ðŸ“„ File: [`docs/architecture/ETL-IMPLEMENTATION-GUIDE.md`](docs/architecture/ETL-IMPLEMENTATION-GUIDE.md)

**600 lines with working code:**
- Database migration code (alembic)
- Base transformer framework (abstract classes)
- Lineage recorder utility
- Source transformers (PFF, CFR, NFL) with complete Python code
- ETL orchestrator (async orchestration patterns)
- Monitoring & data quality queries
- Step-by-step implementation (3 phases)
- Deployment checklist

**Immediate Use:**
- Copy migration code to `migrations/versions/`
- Use base transformer as template for each source
- Deploy orchestrator using provided pseudocode

### 3. **Architecture Rationale**
ðŸ“„ File: [`docs/ETL-ARCHITECTURE-RATIONALE.md`](docs/ETL-ARCHITECTURE-RATIONALE.md)

**400 lines explaining:**
- Problem statement (current limitations)
- Why ETL vs. other approaches
- 3 design principles (immutability, single-source-of-truth, auditability)
- Worked example (Alex Johnson prospect across 4 sources)
- Storage/performance trade-offs with mitigation
- Risk mitigation strategies
- Success metrics
- Leadership questions

**Best For:**
- Stakeholder presentations
- Product manager understanding
- Engineering leadership buy-in
- Technical decision justification

### 4. **Quick Start Guide**
ðŸ“„ File: [`docs/ETL-QUICK-START.md`](docs/ETL-QUICK-START.md)

**250 lines quick reference:**
- 5-minute overview
- 3 key concepts (staging, canonical, lineage)
- Daily pipeline flow (6 steps)
- Key tables summary
- Common questions answered
- Example: Adding new source
- File reading order

**Best For:**
- Team onboarding
- Quick reference during development
- Explaining to stakeholders
- Interview preparation

### 5. **Visual Reference**
ðŸ“„ File: [`docs/ETL-VISUAL-REFERENCE.md`](docs/ETL-VISUAL-REFERENCE.md)

**400 lines of diagrams:**
- Complete data flow architecture (ASCII diagrams)
- Prospect deduplication walkthrough
- Table relationships (ER-style)
- Conflict resolution flow
- Pipeline execution timeline
- Data quality dashboard queries
- Schema evolution example

**Best For:**
- Whiteboarding
- Presentations
- Understanding flow visually
- Teaching new team members

### 6. **Architecture Index & Summary**
ðŸ“„ File: [`docs/ETL-ARCHITECTURE-INDEX.md`](docs/ETL-ARCHITECTURE-INDEX.md)

**Comprehensive index with:**
- Overview of all documents
- Architecture quick reference
- Implementation phases
- Risk & mitigation matrix
- Success criteria
- Stakeholder questions
- Sign-off checklist

---

## Key Design Decisions

### âœ… Staging Tables (Raw Data, Immutable)
Each source gets dedicated staging table:
```sql
pff_staging:        grade 0-100, position_grade, snaps_analyzed
nfl_combine_staging: height "6-2", weight 310, forty_time 4.82
cfr_staging:        passing_yards 3500, tackles 120, season 2024
yahoo_staging:      ranking #12, analyst_grade, comps
```

**Why:** Enables reprocessing without re-extracting. If transformation rule changes, re-run transform phase. Original data frozen = auditable + replayable.

### âœ… Canonical Models (Single Source of Truth)
Deduplicated, quality-checked tables:
```sql
prospect_core:          Master prospect identity (merged from 5 sources)
prospect_grades:        Multi-source grades (PFF + Yahoo + ESPN all kept)
prospect_measurements:  Reconciled physical tests (conflict-resolved)
prospect_college_stats: Position-normalized college performance
```

**Why:** Single source of truth for API + analytics. Source attribution maintained in data_lineage table.

### âœ… Data Lineage (Complete Audit Trail)
Every field change tracked:
```sql
"Where did Alex Johnson's height=74 come from?"
â†’ data_lineage shows: NFL Combine reported 74 on 2026-02-14
  PFF reported 73 on 2026-02-13 (OVERWROTE using "most_recent" rule)
```

**Why:** Enables debugging, compliance, and data quality analysis. Can reprocess with different rules.

### âœ… Parallel Extraction (30 seconds)
All sources extracted concurrently (not sequentially):
```
PFF: 15s  â”œâ”€â†’ pff_staging
NFL: 20s  â”œâ”€â†’ nfl_combine_staging  (longest = total time)
CFR: 25s  â”œâ”€â†’ cfr_staging
Yahoo: 8s â””â”€â†’ yahoo_staging
Total: ~25s (vs. 68s if sequential)
```

### âœ… Atomic Loading (All-or-Nothing)
Single database transaction for entire load phase:
```
BEGIN TRANSACTION
  INSERT prospect_core
  INSERT prospect_grades
  INSERT prospect_measurements
  INSERT prospect_college_stats
  INSERT data_lineage
COMMIT (all succeed or all rollback)
```

**Why:** Data consistency. If any insert fails, entire load rolls back.

---

## Architecture Layers Explained

```
Layer 1: EXTRACTION         Data pulled from external sources
                           (PFF, NFL.com, CFR, Yahoo, ESPN)
                           â†“
Layer 2: STAGING            Raw data stored exactly as received
                           (pff_staging, nfl_staging, cfr_staging, ...)
                           â†“
Layer 3: TRANSFORMATION     Cleansing, normalization, matching
                           (grade 0-100â†’5-10, name fuzzy match, ...)
                           â†“
Layer 4: CANONICAL          Merged, deduplicated, quality-checked
                           (prospect_core, grades, measurements, ...)
                           â†“
Layer 5: LINEAGE            Complete audit trail (data_lineage table)
                           â†“
Layer 6: ANALYTICS/API      Materialized views + REST endpoints
                           (prospect_quality_scores, API queries)
```

**Key Property:** Each layer is independent. Can reprocess through layer 3 without re-extracting (layer 1).

---

## Storage & Performance Analysis

### Storage Overhead: ~3x Raw Data
```
Raw staging:    100 MB
Canonical:       50 MB
Lineage:        200 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:          350 MB

Mitigation:
  â€¢ Archive staging >90 days (keep ~100 MB active)
  â€¢ Partition lineage table by date
  â€¢ Compress old lineage quarterly
```

### Pipeline Duration: ~2 minutes
```
Extract:    30 sec (parallel)
Validate:   10 sec
Transform:  45 sec (batch processing)
Load:       20 sec (atomic transaction)
Publish:    10 sec (refresh views)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:     ~115 sec (acceptable for daily batch)

Future optimization:
  â€¢ Caching (if source unchanged, skip extract)
  â€¢ Incremental transform (only changed records)
  â€¢ Parallel loading (separate DB transactions per table)
  â†’ Target: <1 minute
```

### Query Performance
```
Canonical models:  FAST (indexed, normalized)
Lineage queries:   SLOW (large table, OLAP queries)
                   Solution: Separate analytical DB (future)
```

---

## Implementation Roadmap

### Phase 1: Foundation (Weeks 1-2)
- Create staging tables (PFF, NFL, CFR, Yahoo)
- Create canonical tables (prospect_core, grades, measurements, stats)
- Create lineage audit table
- Build base transformer framework
- Build lineage recorder utility

**Owner:** Data Engineer + Backend Engineer  
**Deliverable:** Database schema + base classes ready for transformers

### Phase 2: Source Transformers (Weeks 2-3)
- Implement PFF transformer (grades normalization)
- Implement CFR transformer (college stats validation)
- Implement NFL transformer (measurements parsing)
- Implement conflict resolution logic
- Build prospect deduplication algorithm

**Owner:** Data Engineer  
**Deliverable:** All transformers working, tested, documented

### Phase 3: Orchestration (Week 3-4)
- Build ETL orchestrator
- Implement parallel extraction
- Build atomic transaction handling
- Deploy APScheduler daily pipeline
- Create monitoring dashboards

**Owner:** Data Engineer  
**Deliverable:** End-to-end pipeline running daily, monitored

### Phase 4: Operations (Ongoing)
- Data quality scorecard
- Operational runbooks
- Performance optimization
- Add new sources (ESPN, etc.)
- Advanced analytics features

**Owner:** Entire team  
**Deliverable:** Production-ready operations, documented processes

---

## Success Criteria (4-5 Weeks)

1. **Data Integration** âœ“
   - PFF + NFL + CFR data merged into single prospect records
   - >95% prospects matched across sources
   - prospect_core has multi-source IDs for 95%+ of records

2. **Auditability** âœ“
   - 100% of fields traceable to source
   - Can answer "where did this value come from?" in 1 SQL query
   - Complete lineage history for compliance

3. **Scalability** âœ“
   - Can add new source (e.g., ESPN) in <1 day
   - Zero schema changes to canonical layer
   - No deduplication logic changes needed

4. **Data Quality** âœ“
   - Automatic conflict detection when sources disagree
   - Data quality scorecard shows completeness by source
   - Transformation validation catches 99%+ of errors

5. **Operations** âœ“
   - Pipeline >99% success rate (retries)
   - <1 hour MTTR for data quality issues
   - Comprehensive monitoring + alerts
   - Runbooks documented

---

## Risks & Mitigation

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Increased storage (3x) | Database growth | Archive staging >90 days, partition by date |
| Pipeline latency (2 min) | Data freshness | Optimize transforms, parallel processing, caching |
| Transformation bugs | Data regression | Comprehensive unit tests, staging validation, rollout gradual |
| Team skill gap | Slow development | ETL training, pair programming, documented code |
| Lineage query perf | Slow analytics | Archive old data, separate analytical DB (future) |
| **Overall Risk Level** | **MEDIUM** | **Well-mitigated by design** |

---

## Why This Architecture?

### For Scalability
Adding new source = new staging table + transformer. Zero schema changes.
Can manage 10+ sources with same framework.

### For Auditability
Complete lineage shows: source â†’ staging â†’ transformation â†’ canonical
Every decision documented (most_recent vs. priority_order conflict resolution)
Complies with data governance + regulatory audit requirements

### For Data Quality
- Staging validation catches malformed data early
- Transformation testing isolates bugs
- Conflict detection explicit when sources disagree
- Lineage enables debugging to root cause

### For Operational Flexibility
- Can reprocess data through new rules without re-extracting
- Can rollback to previous version using lineage
- Easy debugging: trace quality issues to source
- Reproducible transformations (code â†’ version control)

### For Analytics
- Lineage data enables data quality scorecards
- Source comparison metrics ("Which source has best data?")
- Confidence scoring for ML features
- Historical analysis (how data quality improved over time)

---

## Questions for Leadership

### For Engineering
1. **Timeline:** Can we dedicate 4-5 weeks for proper foundation vs. quick-and-dirty?
2. **Team:** Do we need to hire ETL engineer or train existing backend team?
3. **Future:** Real-time CDC (Kafka) now, or stick with daily batch?

### For Product
1. **Data Completeness:** Is 95% deduplication accuracy acceptable for 2026 draft?
2. **Feature Priority:** Which analytical queries are most important? (roadmapping)
3. **Regulatory:** Any audit/compliance requirements for data lineage?

### For Operations
1. **Monitoring:** What's acceptable error rate? Alert thresholds?
2. **Deployment:** Can we afford 2-minute pipeline? Or need <1 minute?
3. **Retention:** How long keep staging data for audit/reprocessing?

---

## Next Steps

1. **Schedule Architecture Review Meeting**
   - Present to engineering leadership + product
   - Get alignment on 4-5 week timeline
   - Assign team

2. **Start Phase 1 (Immediately)**
   - Data engineer: Create migrations + base classes
   - Backend engineer: Review schema, finalize canonical models
   - Parallel: Backend starts on API endpoints

3. **Build Incrementally**
   - Phase 1: Foundation complete + tested
   - Phase 2: Start with PFF transformer, then CFR
   - Phase 3: Orchestrator + monitoring
   - Phase 4: Optimize + add new sources

4. **Deploy Carefully**
   - Staging environment validation (full pipeline run)
   - Production deployment with monitoring
   - Run 3 successful pipelines before considering "done"

---

## Document Map

| Document | Purpose | Audience | Read Time |
|----------|---------|----------|-----------|
| [ADR-0011](docs/architecture/0011-etl-multi-source-architecture.md) | Technical design, complete spec | Engineers, architects | 45 min |
| [Implementation Guide](docs/architecture/ETL-IMPLEMENTATION-GUIDE.md) | Step-by-step code | Backend/data engineers | 60 min |
| [Rationale](docs/ETL-ARCHITECTURE-RATIONALE.md) | Business justification | Leadership, PM | 30 min |
| [Quick Start](docs/ETL-QUICK-START.md) | Team reference | All engineers | 5 min |
| [Visual Reference](docs/ETL-VISUAL-REFERENCE.md) | Diagrams + examples | Visual learners | 20 min |
| [Index](docs/ETL-ARCHITECTURE-INDEX.md) | Navigation + overview | All stakeholders | 10 min |

---

## Approval Checklist

- [ ] CTO / VP Engineering: Approve timeline and resource allocation
- [ ] Product Manager: Approve data quality trade-offs
- [ ] Data Engineering Lead: Confirm team capacity
- [ ] Backend Engineering Lead: Confirm API design
- [ ] Security/Compliance: Confirm lineage meets audit requirements

---

## Summary

**Problem:** Current system monolithic, not scalable, not auditable

**Solution:** Enterprise-grade ETL with:
- Source-specific staging (raw data capture)
- Canonical transformation layer (normalized entities)
- Complete lineage tracking (audit trail)
- Parallel extraction + atomic loading

**Result:** Scalable, auditable, maintainable system

**Timeline:** 4-5 weeks to production

**Team:** Data Engineer + Backend Engineer

**Confidence Level:** â­â­â­â­â­ (Well-designed, proven patterns)

---

**Status:** Architecture Design Complete âœ…  
**Ready For:** Engineering implementation  
**Last Updated:** February 15, 2026  
**Version:** 1.0 (Final Architecture)

---

All documentation is in `docs/` directory. Start with **ETL-QUICK-START.md**, then read architecture documents based on your role.

ðŸš€ **Ready to build. Assign team. Start Phase 1.**
